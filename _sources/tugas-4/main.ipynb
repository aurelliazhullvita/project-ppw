{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Word embedding\n",
        "\n",
        "### Skip-Gram\n",
        "\n",
        "### CROW (continues Bag of word)\n",
        "\n",
        "Skip Gram merupakan sebuah sebuah arsitektur dalam menentukan nilai vector suatu kata dengan memprediksi kata-kata yang mungkin muncul jika diberikan suatu kata sebagai input-nya.\n",
        "\n",
        "Model Skip-Gram adalah pendekatan jaringan saraf yang digunakan dalam pemrosesan bahasa alami untuk memprediksi konteks (kata-kata di sekitarnya) dari kata target tertentu. Model ini berfokus pada memaksimalkan kemungkinan kata konteks muncul di sekitar kata target.\n",
        "\n",
        "**Tujuan :**\n",
        "Model Skip-Gram bertujuan untuk memprediksi kata-kata kontekstual yang mengelilingi kata target tertentu dalam jendela tertentu. Misalnya, dalam kalimat \"Kucing duduk di atas tikar,\" jika \"kucing\" adalah kata target, model akan mencoba memprediksi kata-kata di sekitarnya seperti \"Kucing\", \"duduk\", dll., dalam ukuran jendela yang ditentukan.\n",
        "\n",
        "**Arsitektur :**\n",
        "* Lapisan Input : Vektor yang dikodekan one-hot mewakili kata target.\n",
        "\n",
        "* Lapisan Tersembunyi : Lapisan tersembunyi yang padat mengubah vektor masukan menjadi representasi berdimensi lebih rendah, yang umumnya disebut sebagai penyisipan kata.\n",
        "\n",
        "* Output Layer : Jaringan memprediksi kata konteks berdasarkan embedding yang dihasilkan oleh hidden layer. Kata konteks ini juga direpresentasikan sebagai distribusi probabilitas menggunakan fungsi softmax.\n",
        "\n",
        "**Pelatihan :**\n",
        "Selama pelatihan, model Skip-Gram memperbarui penyertaan dengan meminimalkan kerugian (seringkali melalui Pengambilan Sampel Negatif, yang mengurangi kerumitan komputasi dengan berfokus pada prediksi yang salah).\n",
        "Proses ini melibatkan propagasi maju untuk memprediksi kata konteks dan propagasi balik untuk menyesuaikan bobot berdasarkan kesalahan antara kata konteks yang diprediksi dan aktual.\n",
        "* Penanaman Kata : Model Skip-Gram menghasilkan penanaman kata berkualitas tinggi di mana kata-kata dengan makna yang serupa memiliki representasi yang serupa dalam ruang vektor. Misalnya, \"kucing\" dan \"anjing\" mungkin berdekatan satu sama lain dalam ruang penanaman karena keduanya adalah hewan.\n",
        "\n",
        "* Skip-Gram vs CBOW : Skip-Gram berbeda dari Continuous Bag of Words (CBOW). Sementara CBOW memprediksi kata target dari kata konteks di sekitarnya, Skip-Gram bekerja secara terbalik dengan memprediksi kata konteks dari kata target.\n",
        "\n",
        "* Tantangan : Meskipun efektif, Skip-Gram dapat memakan banyak biaya komputasi ketika kosakatanya besar, memerlukan teknik seperti pengambilan sampel negatif agar prosesnya efisien.\n",
        "\n",
        "Berikut adalah contoh input dan output untuk representasi vektor kata menggunakan model Word2Vec atau Skip-Gram:\n",
        "\n",
        "**Input:**\n",
        "contoh pemilihan kata input misalnya :\n",
        "\"saya suka bermain sepak bola di lapangan besar\"\n",
        "\n",
        "langkah - langkah yang dilakukan oleh model Word2Vec dengan pendekatan Skip-Gram:\n",
        "1. Tokenisasi: Kalimat tersebut akan diubah menjadi kumpulan kata:\n",
        "    \n",
        "    [\"saya\", \"suka\", \"bermain\", \"sepak\", \"bola\", \"di\", \"lapangan\", \"besar\"]\n",
        "\n",
        "2. Window Size: Untuk setiap kata, kita akan memilih konteks kata di sekitarnya dengan jendela tertentu, misalnya jendela ukuran 2 (dua kata sebelum dan sesudah).\n",
        "\n",
        "3. Pasangan kata (word pairs): Model Skip-Gram bertujuan untuk memprediksi konteks dari kata pusat (target). Pasangan yang dihasilkan dari kalimat dengan jendela 2 adalah:\n",
        "\n",
        "    (\"saya\", \"suka\"), (\"saya\", \"bermain\")\n",
        "(\"suka\", \"saya\"), (\"suka\", \"bermain\"), (\"suka\", \"sepak\")\n",
        "(\"bermain\", \"suka\"), (\"bermain\", \"sepak\"), (\"bermain\", \"bola\")\n",
        "(\"sepak\", \"bermain\"), (\"sepak\", \"bola\"), (\"sepak\", \"di\")\n",
        "(\"bola\", \"sepak\"), (\"bola\", \"di\"), (\"bola\", \"lapangan\")\n",
        "(\"di\", \"bola\"), (\"di\", \"lapangan\"), (\"di\", \"besar\")\n",
        "(\"lapangan\", \"di\"), (\"lapangan\", \"besar\")\n",
        "(\"besar\", \"lapangan\"), (\"besar\", \"di\")\n",
        "\n",
        "4. Setelah model Skip-Gram dilatih, kita mendapatkan representasi vektor untuk setiap kata.\n",
        "Pemilihan ukuran jendela yang tepat tergantung pada beberapa faktor:\n",
        "\n",
        "**Output:**\n",
        "Model Word2Vec dengan pendekatan Skip-Gram menghasilkan vektor berdimensi tinggi untuk setiap kata dalam kamus (vocabulary) berdasarkan kemunculan bersama mereka dalam konteks tertentu.\n",
        "\n",
        "Misalkan kita ingin melihat representasi vektor dari kata \"sepak\", outputnya bisa berupa vektor seperti ini (dengan dimensi, misalnya, 100):\n",
        "\n",
        "    \"sepak\": [0.423, -0.237, 0.101, ..., 0.054, -0.134, 0.876]\n",
        "\n",
        "Vektor ini adalah representasi kata \"sepak\" dalam ruang vektor berdimensi tinggi, yang dapat digunakan untuk menghitung kesamaan antar kata (misalnya menggunakan cosine similarity).\n",
        "\n",
        "**Contoh Kesamaan Kata:**\n",
        "jika ingin menghitung kesamaan antara kata \"sepak\" dan \"bola\", maka model akan menghasilkan nilai kesamaan tertentu. Misalnya:\n",
        "\n",
        "    similarity(\"sepak\", \"bola\") = 0.89\n",
        "\n",
        "Nilai ini menunjukkan bahwa kedua kata tersebut cukup mirip dalam konteks tertentu, seperti yang dipelajari oleh model.\n",
        "\n",
        "\n",
        " rumus-rumus yang terkait:\n",
        "1. Turunan fungsi biaya\n",
        "$$\n",
        "\\underset{\\theta}{\\text{argmax}} \\,\\, p(w_{1}, w_{2}, ... , w_{C}|w_{center}; \\, \\theta) \\tag{1}\n",
        "$$\n",
        "\n",
        " *penjelasan rumus di atas:*\n",
        "\n",
        "- simbol argmax𝜃 menunjukkan bahwa ingin menemukan nilai parameter 𝜃 memaksimalkan fungsi probabilitas. dalam konteks ini sedang mencari parameter 𝜃 yang memaksimalkan probabilitas kata - kata konteks w1,w2,....,Wc yang muncul berdasarkan kata pusat Wcenter\n",
        "- probabilitas bersyarat p(w1,w2,...Wc|wcenter;𝜃) dari kata kata konteks w1,w2,...wc yang muncul berdasarkan kata pusat wcenter, dengan parameter 𝜃.\n",
        "- kata pusat dan kata konteks, Wcenter adalah kata pusat (kata yang fokus atau acuan). W1,W2,....Wc adalah kata kata konteks yaitu kata kata yang muncul di sekitar kata pusat dalam jendela konteks tertentu.\n",
        "- parameter 𝜃\n",
        "θ mewakili parameter model yang sedang di optimalkan. word2Vec (terutama pendekatan skip-gram),parameter 𝜃\n",
        "adalah embedding vektor atau representasi vektor dari kata - kata dalam ruang berdimensi tinggi.\n",
        "- skip gram model dari word2Vec,tujuan utamanya adalam untuk memprediksi kata kata konteks W1,W2,...,Wc berdasarkan kata pusat Wcenter. dalam rumus ini, kita mencoba menemukan parameter 𝜃 (vektor kata) yang memaksimalkan probabilitas bahwa kata kata konteks yang ada di sekitar kata pusat\n",
        "- seperti contoh di atas, jika kata pusat adalah \"sepak\", model mencoba memaksimalkan probabilitas bahwa kata-kata konteks seperti \"bola\", \"lapangan\", atau \"tim\" muncul di sekitarnya. Dengan melatih parameter\n",
        "𝜃 (embedding vektor) model berusaha mempelajari hubungan antara kata-kata tersebut.\n",
        "\n",
        "**Forward Propagation: Hidden (Projection) Layer (h)**\n",
        "![image](https://hackmd.io/_uploads/Hy0vpdHR0.png)\n",
        "Dalam konsep Natural Language Prosessing hidden layer dapat disebut sebagai projection layer. Dikarenakan h\n",
        "itu sebenarnya $N-$dim projected vector dari one hot encoding dari input vektor h\n",
        "\n",
        "$\\begin{align*} h = W_{input}^T \\cdot x \\in \\mathbb{R}^{N} \\end{align*}$\n",
        "\n",
        "** Forward Propagation: Softmax Output Layer**\n",
        "Output layer berisi probabilitas dari semua kata unik dalam corpus (${V-}$dim). Untuk menentukan nilai probabilitas diperlukan kata konteks dan kata target\n",
        "\n",
        "$\\begin{align*} p(w_{context}|w_{center}) = \\frac{exp(W_{output_{(context)}} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}^{1} \\end{align*}$\n",
        "\n",
        "- Woutput(i) adalah nilai elemen vektor output dari kata ke-${i}$ yang memiliki ukuran 1×N\n",
        "- woutput context adalah nilai elemen vektor output dari kata konteks yang juga memiliki ukuran 1×N\n",
        "- V adalah total kata unik dari corpus\n",
        "- h adalah hidden (projection) layer yang memiliki ukuran (N×1)\n",
        "- Output yang dihasilkan adalah nilai skalar dengan ukuran(1×1)yang memiliki probabilitas dengan rentang [0,1]\n",
        "\n",
        "Proses ini akan melakukan perulangan sejumlah V kali sesuai dengan jumlah kata unik dalam corpus dengan kata target.\n",
        "\n",
        "$\\begin{align*} \\left[\\begin{array}{c} p(w_{1}|w_{center}) \\ p(w_{2}|w_{center}) \\ p(w_{3}|w_{center}) \\ \\vdots \\ p(w_{V}|w_{center}) \\end{array} \\right] = \\frac{exp(W_{output} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}^{V} \\end{align*}$\n",
        "\n",
        "Sehingga Woutput yang memiliki ukuran(V×N) akan dikalikan dengan h yang memiliki ukuran (N×1)sehingga menghasilkan dot product vector yang memiliki ukuran (V×1).Terakhir, dot product vector akan dimasukkan ke dalam fungsi aktivasi softmax.\n",
        "![image](https://hackmd.io/_uploads/SJFE1cr0R.png)\n",
        "Hasil nilai dari aktivasi softmax jika dijumlahkan semua akan bernilai 1.\n",
        "\n",
        "**Backward Propagation: Prediction Error**\n",
        "Skip-Gram model melakukan optimasi bobot matriks (θ) dengan mengurangi prediction error. Prediction error merupakan perbedaan antara hasil dari softmax output layer (ypred) dengan probabilitas target yang sebenarnya (ytrue) dari setiap kata. ytrue merupakan vektor one hot encoder dari setiap kata konteks.\n",
        "![image](https://hackmd.io/_uploads/SJ5lxcH0C.png)\n",
        " terdapat 2 kata yaitu \"the\" dan \"who\" yang telah memiliki hasil dari pengurangan\n",
        "ypred dan ytrue. Selanjutnya dilakukan penjumlahan untuk melihat bobot yang kemudian akan dimasukkan/diupdate pada matriks bobot yang ada.\n",
        "![image](https://hackmd.io/_uploads/B1gNxcHRA.png)\n",
        "Update matriks bobot terus dilakukan hingga mendapatkan prediction error paling kecil.\n",
        "![image](https://hackmd.io/_uploads/H1oSxqS0R.png)\n",
        "\n",
        "**Demonstrasi Perhitungan**\n",
        "1. Mencari nilai hidden layer (h):\n",
        "![image](https://hackmd.io/_uploads/Syc3l5S0C.png)\n",
        "2. Menghitung output layer dengan fungsi aktivasi softmax:\n",
        "![image](https://hackmd.io/_uploads/HkwZW5BAC.png)\n",
        "3. Menjumlahkan prediction error dari kata konteks:\n",
        "![image](https://hackmd.io/_uploads/SymQW9BAR.png)\n",
        "4. Mengitung ∇Winput :\n",
        "rumus yang dapat direpresentasikan juga dengan cara mengalikan vektor one hot encoding dari input layer (x) dengan WoutputT∑c=1Cec. Rumus WoutputT∑c=1Cec merupakan hasil perkalian antara (Woutput) dengan penjumlahan prediction error (∑c=1Cec).\n",
        "$\\begin{align*} \\frac{\\partial J}{\\partial W_{input}} = x \\cdot (W_{output}^T \\sum^C_{c=1} e_c) \\end{align*}$\n",
        "![image](https://hackmd.io/_uploads/BkMfzqrC0.png)\n",
        "5. Menghitung ∇Woutput :\n",
        "yang dapat direprenstasikan juga dengan cara mengalikan hidden layer (h) dengan penjumlahan prediction error (∑c=1Cec). Tidak sama dengan matriks bobot input (Winput) yang hanya mengupdate satu nilai vektor, tetapi semua nilai vektor pada matriks bobot output (Woutput) akan diperbarui.\n",
        "$\\begin{align*} \\frac{\\partial J}{\\partial W_{output}} = h \\cdot \\sum^C_{c=1} e_c \\end{align*}$\n",
        "![image](https://hackmd.io/_uploads/r1Kif9BRC.png)\n",
        "6. Memperbarui matriks bobot input dan output :\n",
        "Matriks bobot input dan output akan diperbarui menggunakan rumus\n",
        "- $\\begin{align*} W_{input}^{(new)}=W_{input}^{(old)}- \\eta \\cdot x \\cdot (W_{output}^T \\sum^C_{c=1} e_c) \\end{align*}$\n",
        "![image](https://hackmd.io/_uploads/Hy-Jm5SA0.png)\n",
        "- $\\begin{align*} W_{output}^{(new)}=W_{output}^{(old)}- \\eta \\cdot h \\cdot \\sum^C_{c=1} e_c \\end{align*}$\n",
        "![image](https://hackmd.io/_uploads/BJJMQ9SC0.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jlU18qWkqgrO"
      }
    }
  ]
}